[net]
# Training
batch=128
subdivisions=2

# Testing
# batch=1
# subdivisions=1

height=416
width=416
channels=3
min_crop=128
max_crop=448

burn_in=1000
learning_rate=0.1
policy=poly
power=4
max_batches=800000
momentum=0.9
decay=0.0005

angle=7
hue=.1
saturation=.75
exposure=.75
aspect=.75

#################### before downsample #####################

# conv1
# layer-0
[convolutional]
filters=32
size=3
pad=1
stride=2
batch_normalize=1
activation=relu

#################### after downsample #####################

# conv2_1_expand
# layer-1
[convolutional]
filters=32
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv2_1_dwise
# layer-2
[convolutional]
groups=32
filters=32
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv2_1_linear
# layer-3
[convolutional]
filters=16
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# conv2_2_expand
# layer-4
[convolutional]
filters=96
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

#################### before downsample #####################

# conv2_2_dwise
# layer-5
[convolutional]
groups=96
filters=96
size=3
pad=1
stride=2
batch_normalize=1
activation=relu

#################### after downsample #####################



# conv2_2_linear
# layer-6
[convolutional]
filters=24
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# conv3_1_expand
# layer-7
[convolutional]
filters=144
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv3_1_dwise
# layer-8
[convolutional]
groups=144
filters=144
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv3_1_linear
# layer-9
[convolutional]
filters=24
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_3_1
# layer-10
[shortcut]
from=-4
activation=linear

# conv_3_2_expand
# layer-11
[convolutional]
filters=144
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

#################### before downsample #####################

# conv_3_2_dwise
# layer-12
[convolutional]
groups=144
filters=144
size=3
pad=1
stride=2
batch_normalize=1
activation=relu

#################### after downsample #####################


# conv_3_2_linear
# layer-13
[convolutional]
filters=32
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# conv_4_1_expand
# layer-14
[convolutional]
filters=192
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_4_1_dwise
# layer-15
[convolutional]
groups=192
filters=192
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_1_linear
# layer-16
[convolutional]
filters=32
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_4_1
# layer-17
[shortcut]
from=-4
activation=linear

# conv_4_2_expand
# layer-18
[convolutional]
filters=192
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_4_2_dwise
# layer-19
[convolutional]
groups=192
filters=192
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_2_linear
# layer-20
[convolutional]
filters=32
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_4_2
# layer-21
[shortcut]
from=-4
activation=linear

# conv_4_3_expand
# layer-22
[convolutional]
filters=192
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_4_3_dwise
# layer-23
[convolutional]
groups=192
filters=192
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_3_linear
# layer-24
[convolutional]
filters=64
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# conv_4_4_expand
# layer-25
[convolutional]
filters=384
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_4_4_dwise
# layer-26
[convolutional]
groups=384
filters=384
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_4_linear
# layer-27
[convolutional]
filters=64
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_4_4
# layer-28
[shortcut]
from=-4
activation=linear

# conv_4_5_expand
# layer-29
[convolutional]
filters=384
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_4_5_dwise
# layer-30
[convolutional]
groups=384
filters=384
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_5_linear
# layer-31
[convolutional]
filters=64
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_4_5
# layer-32
[shortcut]
from=-4
activation=linear

# conv_4_6_expand
# layer-33
[convolutional]
filters=384
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_4_6_dwise
# layer-34
[convolutional]
groups=384
filters=384
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_6_linear
# layer-35
[convolutional]
filters=64
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_4_6
# layer-36
[shortcut]
from=-4
activation=linear

# conv_4_7_expand
# layer-37
[convolutional]
filters=384
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

#################### before downsample #####################

# conv_4_7_dwise
# layer-38
[convolutional]
groups=384
filters=384
size=3
pad=1
stride=2
batch_normalize=1
activation=relu

#################### after downsample #####################


# conv_4_7_linear
# layer-39
[convolutional]
filters=96
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# conv_5_1_expand
# layer-40
[convolutional]
filters=576
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_5_1_dwise
# layer-41
[convolutional]
groups=576
filters=576
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_5_1_linear
# layer-42
[convolutional]
filters=96
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_5_1
# layer-43
[shortcut]
from=-4
activation=linear

# conv_5_2_expand
# layer-44
[convolutional]
filters=576
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_5_2_dwise
# layer-45
[convolutional]
groups=576
filters=576
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_5_2_linear
# layer-46
[convolutional]
filters=96
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_5_2
# layer-47
[shortcut]
from=-4
activation=linear

# conv_5_3_expand
# layer-48
[convolutional]
filters=576
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

#################### before downsample #####################

# conv_5_3_dwise
# layer-49
[convolutional]
groups=576
filters=576
size=3
pad=1
stride=2
batch_normalize=1
activation=relu

#################### after downsample #####################

# conv_5_3_linear
# layer-50
[convolutional]
filters=160
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# conv_6_1_expand
# layer-51
[convolutional]
filters=960
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_6_1_dwise
# layer-52
[convolutional]
groups=960
filters=960
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_6_1_linear
# layer-53
[convolutional]
filters=160
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_6_1
# layer-54
[shortcut]
from=-4
activation=linear

# conv_6_2_expand
# layer-55
[convolutional]
filters=960
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_6_2_dwise
# layer-56
[convolutional]
groups=960
filters=960
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_6_2_linear
# layer-57
[convolutional]
filters=160
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_6_2
# layer-58
[shortcut]
from=-4
activation=linear

# conv_6_3_expand
# layer-59
[convolutional]
filters=960
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_6_3_dwise
# layer-60
[convolutional]
groups=960
filters=960
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_6_3_linear
# layer-61
[convolutional]
filters=320
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# conv_6_4
# layer-62
# FPN-P5
[convolutional]
filters=512
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# [gap]
# size=1 

##################################

# layer-63
[route]
layers = -2

# layer-64
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# layer-65
[upsample]
stride=2

# layer-66
[route]
layers = -1, 37

# layer-67
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# layer-68
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky

# layer-69
## FPN-P4
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# layer-70
[upsample]
stride=2

# layer-71
[route]
layers = -1, 11

# layer-72
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# layer-73
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky

# layer-74
## FPN-P3
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# layer=75
[spatialmaxpool]
# 52x52 26x26 13x13
from=74, 69, 62
shape=13, 13, 13
out_plane = 256

# layer=76
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

# layer=77
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky

# layer=78
[convolutional]
size=1
stride=1
pad=1
filters=256
activation=leaky

# layer=79
[se]
# attention, feature
from=62, -1
reduction=4
out_plane=256

# layer=80
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

# layer=81
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky

# layer=82
[convolutional]
size=1
stride=1
pad=1
filters=30
activation=linear

# layer=83
[yolo]
mask = 0,1,2,3,4
anchors = 19,19,  36,22,  31,37,  56,40,  79,62
classes=1
num=5
jitter=.3
ignore_thresh = .7
truth_thresh = 1
random=1